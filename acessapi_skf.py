# -*- coding: utf-8 -*-
"""AcessAPI_SKF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m0wrctqA4mIDudnHGeN80sMFZJTb5AtC

# **BIBLIOTECAS**
"""

from datetime import datetime, timedelta
import pandas as pd
import requests
import urllib3
import json
import jwt

"""# **AUTENTICA√á√ÉO SHEETS**"""

import gspread
from google.oauth2.service_account import Credentials
from gspread_dataframe import set_with_dataframe

# L√™ a vari√°vel de ambiente com o conte√∫do do JSON da conta de servi√ßo
service_account_info = json.loads(os.environ["GOOGLE_SERVICE_ACCOUNT"])

# Define os escopos de acesso (Google Sheets)
SCOPES = [
    'https://www.googleapis.com/auth/spreadsheets',
    'https://www.googleapis.com/auth/drive'
]

# Cria as credenciais usando o conte√∫do do secret
creds = Credentials.from_service_account_info(service_account_info, scopes=SCOPES)

# Autentica no Google Sheets
gc = gspread.authorize(creds)

"""# **DADOS DE ACESSO**"""

# === CONFIGURA√á√ïES GLOBAIS ===
BASE_URL = "http://services.repcenter.skf.com:20446"
URLS = {
    "token": f"{BASE_URL}/token",
    "machines": f"{BASE_URL}/v1/machines",
    "points": f"{BASE_URL}/v1/points"
}

CREDENTIALS = {
    "username": "rep\\fabio.demuner",
    "password": "KTw302@*7AvR^mgK",
    "grant_type": "password"
}

def obter_token():
    response = requests.post(URLS["token"], data=CREDENTIALS, verify=False)
    response.raise_for_status()
    return response.json()["access_token"]

"""# **REQUISI√á√ÉO DE M√ÅQUINAS**"""

def get_machines(token):
    url = URLS["machines"]
    headers = {
        "Authorization": f"Bearer {token}",
        "Accept": "application/json"
    }
    resp = requests.get(url, headers=headers, verify=False)
    resp.raise_for_status()
    return pd.DataFrame(resp.json())

# EXECU√á√ÉO
token = obter_token()
df_machines = get_machines(token)

# === PROCESSAMENTO ===
# Separar a coluna 'path' por barra invertida
df_path_split = df_machines['path'].str.split('\\', expand=True)

# Adiciona prefixo nos nomes das colunas novas
df_path_split.columns = [f"path_{i}" for i in range(df_path_split.shape[1])]

# Junta com o DataFrame original
df_machines_split = pd.concat([df_machines, df_path_split], axis=1)

"""**TABELA E LISTA MACHINE ID**"""

# Filtra linhas onde path_1 == 'UBU' e j√° seleciona colunas desejadas
df_machine = df_machines_split.loc[df_machines_split['path_1'] == 'UBU', ['id', 'name', 'path']].copy()

# Extrai lista com os IDs dos ativos filtrados
machine_ids = df_machine['id'].tolist()

# Exibe a lista
print(f"‚úÖ {len(machine_ids)} ativos encontrados")
print(machine_ids)

"""# **PLANILHA MACHINE ID**"""

# Nome da planilha
nome_da_planilha = "SKF_machineId"
nome_da_aba = "Sheet1"

# Abre a planilha
planilha = gc.open(nome_da_planilha)
aba = planilha.worksheet(nome_da_aba)

# Limpa a aba antes de escrever os dados
aba.clear()

# Envia o DataFrame para a aba
set_with_dataframe(aba, df_machine)

print("Dados enviados com sucesso para o Google Sheets!")

"""# **REQUISI√á√ÉO DE PONTOS**"""

def get_points(token, machine_ids):
    headers = {"Authorization": f"Bearer {token}", "Accept": "application/json"}
    pontos = []

    for mid in machine_ids:
        url = f"{URLS['machines']}/{mid}/points"
        resp = requests.get(url, headers=headers, verify=False)

        if resp.status_code == 200:
            dados = resp.json()
            for p in dados:
                p["MachineId"] = mid
            pontos.extend(dados)

    return pd.DataFrame(pontos)

# EXECU√á√ÉO
token = obter_token()
df_point_raw = get_points(token, machine_ids)

# Seleciona apenas colunas desejadas
colunas_desejadas = ['ID', 'Name', 'NodeTypeName', 'MachineId']
df_point = df_point_raw[colunas_desejadas].copy()

"""# **LISTA DE PONTOS**"""

# Extrai lista com os IDs dos ativos filtrados
point_ids = df_point['ID'].tolist()

# Exibe a lista
print(f"{len(point_ids)} pontos encontrados")
print(point_ids)

"""# **PLANILHA POINT ID**"""

# Nome da planilha
nome_da_planilha = "SKF_pointId"
nome_da_aba = "Sheet1"

# Abre a planilha
planilha = gc.open(nome_da_planilha)
aba = planilha.worksheet(nome_da_aba)

# Limpa a aba antes de escrever os dados (opcional)
aba.clear()

# Envia o DataFrame para a aba
set_with_dataframe(aba, df_point)

print("Dados enviados com sucesso para o Google Sheets!")

"""# **REQUISI√á√ÉO DE ALARMES**"""

def get_alarms(token, machine_ids):
    headers = {"Authorization": f"Bearer {token}", "Accept": "application/json"}
    registros = []

    for mid in machine_ids:
        url = f"{URLS['machines']}/{mid}/points"
        resp = requests.get(url, headers=headers, verify=False)

        if resp.status_code == 200:
            dados_pontos = resp.json()
            for ponto in dados_pontos:
                registro = {
                    "ID": ponto.get("ID"),
                    "HighAlarm": "",
                    "HighWarning": "",
                    "Freq_AlarmLevel": "",
                    "Freq_WarningLevel": ""
                }

                # === OverallAlarm ===
                overall_alarm = ponto.get("OverallAlarm")
                summary = overall_alarm.get("Summary", "") if isinstance(overall_alarm, dict) else ""
                if summary:
                    parts = summary.lower().replace(" / ", "/").split("/")
                    for part in parts:
                        if "high alarm" in part:
                            registro["HighAlarm"] = part.split()[-1]
                        elif "high warning" in part:
                            registro["HighWarning"] = part.split()[-1]

                # === Frequencies["Overall"] ===
                freq_list = ponto.get("Frequencies", [])
                freq_overall = next((f for f in freq_list if f.get("Frequency") == "Overall"), {})
                alarm_raw = str(freq_overall.get("AlarmLevel", ""))
                warning_raw = str(freq_overall.get("WarningLevel", ""))

                registro["Freq_AlarmLevel"] = alarm_raw.split()[0] if alarm_raw else ""
                registro["Freq_WarningLevel"] = warning_raw.split()[0] if warning_raw else ""

                registros.append(registro)

    return pd.DataFrame(registros)

# === EXECU√á√ÉO ===
token = obter_token()
df_alarms = get_alarms(token, machine_ids)

print(f"\n Total de pontos coletados: {len(df_alarms)}")

"""# **PLANILHA ALARMS VALUE**"""

# Nome da planilha
nome_da_planilha = "SKF_alarmsValue"
nome_da_aba = "Sheet1"

# Abre a planilha
planilha = gc.open(nome_da_planilha)
aba = planilha.worksheet(nome_da_aba)

# Limpa a aba antes de escrever os dados (opcional)
aba.clear()

# Envia o DataFrame para a aba
set_with_dataframe(aba, df_alarms)

print("Dados enviados com sucesso para o Google Sheets!")

"""# **REQUISI√á√ÉO DE MEDI√á√ïES**"""

def consultar_trends(point_ids, token):
    headers = {
        "Authorization": f"Bearer {token}",
        "Accept": "application/json"
    }

    from_date = datetime.utcnow() - timedelta(days=1)
    to_date = datetime.utcnow()

    params = {
        "fromDateUTC": from_date.isoformat() + "Z",
        "toDateUTC": to_date.isoformat() + "Z"
    }

    resultados = []
    pontos_com_dados = 0

    for raw_pid in point_ids:
        try:
            pid = int(raw_pid)
        except:
            continue

        url = f"{URLS['points']}/{pid}/trendMeasurements"
        response = requests.get(url, headers=headers, params=params, verify=False)

        if response.status_code == 200:
            data = response.json()
            if data:
                pontos_com_dados += 1
                for record in data:
                    base = {
                        "ReadingTimeUTC": record.get("ReadingTimeUTC"),
                        "PointID": record.get("PointID"),
                        "Speed": record.get("Speed"),
                        "SpeedUnits": record.get("SpeedUnits"),
                        "Process": record.get("Process"),
                        "ProcessUnits": record.get("ProcessUnits"),
                        "Digital": record.get("Digital"),
                        "NumberOfChannels": record.get("NumberOfChannels"),
                    }
                    for m in record.get("Measurements", []):
                        resultados.append({
                            **base,
                            "Channel": m.get("Channel"),
                            "ChannelName": m.get("ChannelName"),
                            "Level": m.get("Level"),
                            "Units": m.get("Units"),
                            "BOV": m.get("BOV")
                        })

    df = pd.DataFrame(resultados)

    print(f"\n‚úÖ Total de pontos com dados: {pontos_com_dados}")
    print(f"üìà Total de medi√ß√µes retornadas: {len(df)}")

    return df

if __name__ == "__main__":
    token = obter_token()
    ubu_point_ids_clean = [int(pid) for pid in point_ids if pd.notna(pid)]

    df_trends_ubu = consultar_trends(ubu_point_ids_clean, token)

    if not df_trends_ubu.empty:
        # Filtra por ChannelName desejados
        df_filtrado = df_trends_ubu[
            df_trends_ubu['ChannelName'].isin(['Valor global', 'Overall'])
        ]

        # Seleciona colunas desejadas
        colunas_desejadas = ['ReadingTimeUTC', 'PointID', 'Level', 'Units']
        df_trendMeasurements = df_filtrado[colunas_desejadas].drop_duplicates(subset=colunas_desejadas)

        print(f"{len(df_trendMeasurements)} medi√ß√µes ap√≥s filtro e remo√ß√£o de duplicados")

    else:
        print("Nenhuma medi√ß√£o retornada.")

"""# **PLANILHA TREND MEASUREMENTS**"""

# Nome da planilha e aba
nome_da_planilha = "SKF_trendMeasurements"
nome_da_aba = "Sheet1"

# Abre a planilha e aba
planilha = gc.open(nome_da_planilha)
aba = planilha.worksheet(nome_da_aba)

# L√™ os dados atuais da aba (j√° existentes)
df_existente = get_as_dataframe(aba, evaluate_formulas=True).dropna(how="all")

# Garante que colunas est√£o no mesmo formato e ordem
colunas_chave = ['ReadingTimeUTC', 'PointID', 'Level', 'Units']
df_existente = df_existente[colunas_chave].dropna()

# Remove duplicados e encontra apenas as linhas novas
df_novos = df_trendMeasurements[~df_trendMeasurements.isin(df_existente.to_dict(orient='list')).all(axis=1)]

# Se houver novos registros, adiciona abaixo
if not df_novos.empty:
    # N√∫mero de linhas j√° existentes (para inserir a partir da pr√≥xima linha vazia)
    ultima_linha = len(df_existente) + 2  # +1 para header, +1 para pr√≥xima
    set_with_dataframe(aba, df_novos, row=ultima_linha, col=1, include_column_header=False)
    print(f"{len(df_novos)} novas medi√ß√µes adicionadas √† planilha!")
else:
    print("Nenhuma medi√ß√£o nova para inserir ‚Äî tudo j√° est√° na planilha.")
