# -*- coding: utf-8 -*-
"""AcessAPI_SKF.ipynb

Automatically generated by Colab.


# **BIBLIOTECAS**
"""

from gspread_dataframe import get_as_dataframe, set_with_dataframe
from datetime import datetime, timedelta, timezone
import pandas as pd
import requests
import urllib3
import json
import jwt
import os

"""# **AUTENTICA√á√ÉO SHEETS**"""

import gspread
from google.oauth2.service_account import Credentials
from gspread_dataframe import set_with_dataframe

# L√™ a vari√°vel de ambiente com o conte√∫do do JSON da conta de servi√ßo
service_account_info = json.loads(os.environ["GOOGLE_SERVICE_ACCOUNT"])

# Define os escopos de acesso (Google Sheets)
SCOPES = [
    'https://www.googleapis.com/auth/spreadsheets',
    'https://www.googleapis.com/auth/drive'
]

# Cria as credenciais usando o conte√∫do do secret
creds = Credentials.from_service_account_info(service_account_info, scopes=SCOPES)

# Autentica no Google Sheets
gc = gspread.authorize(creds)

"""# **DADOS DE ACESSO**"""

# === CONFIGURA√á√ïES GLOBAIS ===
BASE_URL = os.getenv("BASE_URL")
URLS = {
    "token": f"{BASE_URL}/token",
    "machines": f"{BASE_URL}/v1/machines",
    "submachines": f"{BASE_URL}/v1/hierarchy",
    "points": f"{BASE_URL}/v1/points"
}

CREDENTIALS = {
    "username": os.getenv("SKF_USERNAME"),
    "password": os.getenv("SKF_PASSWORD"),
    "grant_type": "password"
}

def obter_token():
    response = requests.post(URLS["token"], data=CREDENTIALS, verify=False)
    response.raise_for_status()
    return response.json()["access_token"]

"""# **REQUISI√á√ÉO DE M√ÅQUINAS**"""

def get_machines(token):
    url = URLS["machines"]
    headers = {
        "Authorization": f"Bearer {token}",
        "Accept": "application/json"
    }
    resp = requests.get(url, headers=headers, verify=False)
    resp.raise_for_status()
    return pd.DataFrame(resp.json())

# EXECU√á√ÉO
token = obter_token()
df_machines = get_machines(token)

# === PROCESSAMENTO ===
# Separar a coluna 'path' por barra invertida
df_path_split = df_machines['path'].str.split('\\', expand=True)

# Adiciona prefixo nos nomes das colunas novas
df_path_split.columns = [f"path_{i}" for i in range(df_path_split.shape[1])]

# Junta com o DataFrame original
df_machines_split = pd.concat([df_machines, df_path_split], axis=1)

"""**TABELA E LISTA MACHINE ID**"""

# Filtra linhas onde path_1 == 'UBU' e j√° seleciona colunas desejadas
df_machine = df_machines_split.loc[df_machines_split['path_1'] == 'UBU', ['id', 'name', 'path']].copy()

# Extrai lista com os IDs dos ativos filtrados
machine_ids = df_machine['id'].tolist()

# Exibe a lista
print(f"{len(machine_ids)} ativos encontrados")
print(machine_ids)

"""# **PLANILHA MACHINE ID**"""

# Nome da planilha
planilha_id = "16Vf3InIX4eW-ZlBnFjerrE-GKkYClgqpz1pBXPKZpVk"
nome_da_aba = "Sheet1"

# Abre a planilha
planilha = gc.open_by_key(planilha_id)
aba = planilha.worksheet(nome_da_aba)

# Limpa a aba antes de escrever os dados
aba.clear()

# Envia o DataFrame para a aba
set_with_dataframe(aba, df_machine)

print("Dados enviados com sucesso para o Google Sheets!")

"""# **REQUISI√á√ÉO DE SUBMACHINE ID**"""

# === OBT√âM A √ÅRVORE COMPLETA ===
def obter_arvore_hierarquia(token):
    url = URLS["submachines"]
    headers = {
        "Authorization": f"Bearer {token}",
        "Accept": "application/json"
    }
    response = requests.get(url, headers=headers, verify=False)
    response.raise_for_status()
    return response.json()

# === EXTRA√á√ÉO DAS SUBMACHINES COM PARENT ===
def get_submachines(data, parent_name=None, submachines=None):
    if submachines is None:
        submachines = []

    for node in data:
        tipo = node.get("typeName")
        nome = node.get("name")
        pai = parent_name

        if tipo == "SubMachine":
            submachines.append({
                "id": node.get("id"),
                "name": nome,
                "description": node.get("description"),
                "parent": pai
            })

        filhos = node.get("children")
        if filhos:
            get_submachines(filhos, parent_name=nome, submachines=submachines)

    return submachines

# === EXECU√á√ÉO ===
token = obter_token()
data_raw = obter_arvore_hierarquia(token)
submachines = get_submachines(data_raw)
df_submachines = pd.DataFrame(submachines)

"""# **PLANILHA SUBMACHINE ID**"""

# Nome da planilha
planilha_id = "1VstU43--BvFijbTJp9XkQZd0Hz6FS1tawzXKiEyx-q4"
nome_da_aba = "Sheet1"

# Abre a planilha
planilha = gc.open_by_key(planilha_id)
aba = planilha.worksheet(nome_da_aba)

# Limpa a aba antes de escrever os dados
aba.clear()

# Envia o DataFrame para a aba
set_with_dataframe(aba, df_submachines)

print("Dados enviados com sucesso para o Google Sheets!")

"""# **REQUISI√á√ÉO DE PONTOS**"""

def get_points(token, machine_ids):
    headers = {"Authorization": f"Bearer {token}", "Accept": "application/json"}
    pontos = []

    for mid in machine_ids:
        url = f"{URLS['machines']}/{mid}/points"
        resp = requests.get(url, headers=headers, verify=False)

        if resp.status_code == 200:
            dados = resp.json()
            for p in dados:
                p["MachineId"] = mid
            pontos.extend(dados)

    return pd.DataFrame(pontos)

# EXECU√á√ÉO
token = obter_token()
df_point_raw = get_points(token, machine_ids)

# Seleciona apenas colunas desejadas
colunas_desejadas = ['ID', 'Name', 'NodeTypeName', 'ParentID','MachineId']
df_point = df_point_raw[colunas_desejadas].copy()

"""# **LISTA DE PONTOS**"""

# Extrai lista com os IDs dos ativos filtrados
point_ids = df_point['ID'].tolist()

# Exibe a lista
print(f"{len(point_ids)} pontos encontrados")
print(point_ids)

"""# **PLANILHA POINT ID**"""

# Nome da planilha
planilha_id = "1KbH3fLvOwuk_pJaf2JgdOQ8dnZ0mXyDvmyN8XUl93b8"
nome_da_aba = "Sheet1"

# Abre a planilha
planilha = gc.open_by_key(planilha_id)
aba = planilha.worksheet(nome_da_aba)

# Limpa a aba antes de escrever os dados (opcional)
aba.clear()

# Envia o DataFrame para a aba
set_with_dataframe(aba, df_point)

print("Dados enviados com sucesso para o Google Sheets!")

"""# **REQUISI√á√ÉO DE ALARMS VALUE**"""

def get_alarms(token, machine_ids):
    import requests
    import pandas as pd

    headers = {
        "Authorization": f"Bearer {token}",
        "Accept": "application/json"
    }
    registros = []

    for mid in machine_ids:
        url = f"{URLS['machines']}/{mid}/points"
        resp = requests.get(url, headers=headers, verify=False)

        if resp.status_code == 200:
            dados_pontos = resp.json()
            for ponto in dados_pontos:
                registro = {
                    "ID": ponto.get("ID"),
                    "HighAlarm": None,
                    "HighWarning": None,
                    "Freq_AlarmLevel": None,
                    "Freq_WarningLevel": None
                }

                # OverallAlarm
                overall_alarm = ponto.get("OverallAlarm")
                summary = overall_alarm.get("Summary", "") if isinstance(overall_alarm, dict) else ""
                if summary:
                    parts = summary.lower().replace(" / ", "/").split("/")
                    for part in parts:
                        if "high alarm" in part:
                            try:
                                val = part.split()[-1].replace(",", ".")
                                registro["HighAlarm"] = float(val)
                            except:
                                registro["HighAlarm"] = None
                        elif "high warning" in part:
                            try:
                                val = part.split()[-1].replace(",", ".")
                                registro["HighWarning"] = float(val)
                            except:
                                registro["HighWarning"] = None

                # Frequencies["Overall"]
                freq_list = ponto.get("Frequencies", [])
                freq_overall = next((f for f in freq_list if f.get("Frequency") == "Overall"), {})

                try:
                    val = str(freq_overall.get("AlarmLevel", "")).split()[0].replace(",", ".")
                    registro["Freq_AlarmLevel"] = float(val)
                except:
                    registro["Freq_AlarmLevel"] = None

                try:
                    val = str(freq_overall.get("WarningLevel", "")).split()[0].replace(",", ".")
                    registro["Freq_WarningLevel"] = float(val)
                except:
                    registro["Freq_WarningLevel"] = None

                registros.append(registro)

    df = pd.DataFrame(registros)
    # For√ßa colunas num√©ricas para float, evita interpretar como datas
    for col in ["HighAlarm", "HighWarning", "Freq_AlarmLevel", "Freq_WarningLevel"]:
        df[col] = pd.to_numeric(df[col], errors='coerce')

    return df

# === EXECU√á√ÉO ===
token = obter_token()
df_alarms = get_alarms(token, machine_ids)

print(f"\nTotal de pontos coletados: {len(df_alarms)}")

"""# **PLANILHA ALARMS VALUE**"""

# Nome da planilha
planilha_id = "1Mik0iRW7WiZYMTOiKF-UboXmbSc7YIN4Yi1AfrpDB2o"
nome_da_aba = "Sheet1"

# Abre a planilha
planilha = gc.open_by_key(planilha_id)
aba = planilha.worksheet(nome_da_aba)

# Limpa a aba antes de escrever os dados (opcional)
aba.clear()

# Envia o DataFrame para a aba
set_with_dataframe(aba, df_alarms)

print("Dados enviados com sucesso para o Google Sheets!")

"""# **REQUISI√á√ÉO DE MEDI√á√ïES**"""

def consultar_trends(point_ids, token):
    headers = {
        "Authorization": f"Bearer {token}",
        "Accept": "application/json"
    }

    from_date = datetime.now(timezone.utc) - timedelta(days=7)
    to_date = datetime.now(timezone.utc)

    params = {
        "fromDateUTC": from_date.isoformat() + "Z",
        "toDateUTC": to_date.isoformat() + "Z"
    }

    resultados = []
    pontos_com_dados = 0

    for raw_pid in point_ids:
        try:
            pid = int(raw_pid)
        except:
            continue

        url = f"{URLS['points']}/{pid}/trendMeasurements"
        response = requests.get(url, headers=headers, params=params, verify=False)

        if response.status_code == 200:
            data = response.json()
            if data:
                pontos_com_dados += 1
                for record in data:
                    base = {
                        "ReadingTimeUTC": record.get("ReadingTimeUTC"),
                        "PointID": record.get("PointID"),
                        "Speed": record.get("Speed"),
                        "SpeedUnits": record.get("SpeedUnits"),
                        "Process": record.get("Process"),
                        "ProcessUnits": record.get("ProcessUnits"),
                        "Digital": record.get("Digital"),
                        "NumberOfChannels": record.get("NumberOfChannels"),
                    }
                    for m in record.get("Measurements", []):
                        resultados.append({
                            **base,
                            "Channel": m.get("Channel"),
                            "Direction": m.get("Direction"),
                            "ChannelName": m.get("ChannelName"),
                            "Level": m.get("Level"),
                            "Units": m.get("Units"),
                            "BOV": m.get("BOV")
                        })

    df = pd.DataFrame(resultados)

    print(f"\n‚úÖ Total de pontos com dados: {pontos_com_dados}")
    print(f"üìà Total de medi√ß√µes retornadas: {len(df)}")

    return df

if __name__ == "__main__":
    token = obter_token()
    ubu_point_ids_clean = [int(pid) for pid in point_ids if pd.notna(pid)]

    df_trends_ubu = consultar_trends(ubu_point_ids_clean, token)

    if not df_trends_ubu.empty:
        # Filtra por ChannelName desejados
        df_filtrado = df_trends_ubu[
            (df_trends_ubu['ChannelName'].isin(['Valor global', 'Overall'])) &
            (df_trends_ubu['Direction'] == "X")
        ]

        # Seleciona colunas desejadas
        colunas_desejadas = ['ReadingTimeUTC', 'PointID', 'Level', 'Units']
        df_trendMeasurements = df_filtrado[colunas_desejadas].drop_duplicates(subset=colunas_desejadas)

        print(f"{len(df_trendMeasurements)} medi√ß√µes ap√≥s filtro e remo√ß√£o de duplicados")
    else:
        print("Nenhuma medi√ß√£o retornada.")

"""# **PLANILHA TREND MEASUREMENTS**"""

# Nome da planilha
planilha_id = "16B1QSbZG-OK8Zs3LiuqG91g_976PcZtmq8kNiVtK4iM"
nome_da_aba = "Sheet1"

# Abre a planilha
planilha = gc.open_by_key(planilha_id)
aba = planilha.worksheet(nome_da_aba)

# L√™ os dados atuais da aba
df_existente = get_as_dataframe(aba, evaluate_formulas=True).dropna(how="all")

# Colunas obrigat√≥rias
colunas_chave = ['ReadingTimeUTC', 'PointID', 'Level', 'Units']

# Se a aba est√° vazia ou n√£o tem as colunas necess√°rias ‚Üí cria cabe√ßalho
if df_existente.empty or not all(col in df_existente.columns for col in colunas_chave):
    aba.clear()
    set_with_dataframe(aba, pd.DataFrame(columns=colunas_chave), row=1, col=1, include_column_header=True)
    df_existente = pd.DataFrame(columns=colunas_chave)

# Garante que colunas est√£o no mesmo formato e ordem
df_existente = df_existente[colunas_chave].dropna()

# Remove duplicados e encontra apenas as linhas novas
df_novos = df_trendMeasurements[~df_trendMeasurements.isin(df_existente.to_dict(orient='list')).all(axis=1)]

# Se houver novos registros, adiciona abaixo
if not df_novos.empty:
    # N√∫mero de linhas j√° existentes (para inserir a partir da pr√≥xima linha vazia)
    ultima_linha = len(df_existente) + 2  # +1 para header, +1 para pr√≥xima
    set_with_dataframe(aba, df_novos, row=ultima_linha, col=1, include_column_header=False)
    print(f"{len(df_novos)} novas medi√ß√µes adicionadas √† planilha!")
else:
    print("Nenhuma medi√ß√£o nova para inserir ‚Äî tudo j√° est√° na planilha.")
