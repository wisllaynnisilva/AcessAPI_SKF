# -*- coding: utf-8 -*-
"""AcessAPI_SKF.ipynb

Automatically generated by Colab.


# **BIBLIOTECAS**
"""

from gspread_dataframe import get_as_dataframe, set_with_dataframe
from datetime import datetime, timedelta
import pandas as pd
import requests
import urllib3
import json
import jwt
import os

"""# **AUTENTICAÇÃO SHEETS**"""

import gspread
from google.oauth2.service_account import Credentials
from gspread_dataframe import set_with_dataframe

# Lê a variável de ambiente com o conteúdo do JSON da conta de serviço
service_account_info = json.loads(os.environ["GOOGLE_SERVICE_ACCOUNT"])

# Define os escopos de acesso (Google Sheets)
SCOPES = [
    'https://www.googleapis.com/auth/spreadsheets',
    'https://www.googleapis.com/auth/drive'
]

# Cria as credenciais usando o conteúdo do secret
creds = Credentials.from_service_account_info(service_account_info, scopes=SCOPES)

# Autentica no Google Sheets
gc = gspread.authorize(creds)

"""# **DADOS DE ACESSO**"""

# === CONFIGURAÇÕES GLOBAIS ===
BASE_URL = os.getenv("BASE_URL")
URLS = {
    "token": f"{BASE_URL}/token",
    "machines": f"{BASE_URL}/v1/machines",
    "points": f"{BASE_URL}/v1/points"
}

CREDENTIALS = {
    "username": os.getenv("SKF_USERNAME"),
    "password": os.getenv("SKF_PASSWORD"),
    "grant_type": "password"
}

def obter_token():
    response = requests.post(URLS["token"], data=CREDENTIALS, verify=False)
    response.raise_for_status()
    return response.json()["access_token"]

"""# **REQUISIÇÃO DE MÁQUINAS**"""

def get_machines(token):
    url = URLS["machines"]
    headers = {
        "Authorization": f"Bearer {token}",
        "Accept": "application/json"
    }
    resp = requests.get(url, headers=headers, verify=False)
    resp.raise_for_status()
    return pd.DataFrame(resp.json())

# EXECUÇÃO
token = obter_token()
df_machines = get_machines(token)

# === PROCESSAMENTO ===
# Separar a coluna 'path' por barra invertida
df_path_split = df_machines['path'].str.split('\\', expand=True)

# Adiciona prefixo nos nomes das colunas novas
df_path_split.columns = [f"path_{i}" for i in range(df_path_split.shape[1])]

# Junta com o DataFrame original
df_machines_split = pd.concat([df_machines, df_path_split], axis=1)

"""**TABELA E LISTA MACHINE ID**"""

# Filtra linhas onde path_1 == 'UBU' e já seleciona colunas desejadas
df_machine = df_machines_split.loc[df_machines_split['path_1'] == 'UBU', ['id', 'name', 'path']].copy()

# Extrai lista com os IDs dos ativos filtrados
machine_ids = df_machine['id'].tolist()

# Exibe a lista
print(f"{len(machine_ids)} ativos encontrados")
print(machine_ids)

"""# **PLANILHA MACHINE ID**"""

# Nome da planilha
planilha_id = "16Vf3InIX4eW-ZlBnFjerrE-GKkYClgqpz1pBXPKZpVk"
nome_da_aba = "Sheet1"

# Abre a planilha
planilha = gc.open_by_key(planilha_id)
aba = planilha.worksheet(nome_da_aba)

# Limpa a aba antes de escrever os dados
aba.clear()

# Envia o DataFrame para a aba
set_with_dataframe(aba, df_machine)

print("Dados enviados com sucesso para o Google Sheets!")

"""# **REQUISIÇÃO DE PONTOS**"""

def get_points(token, machine_ids):
    headers = {"Authorization": f"Bearer {token}", "Accept": "application/json"}
    pontos = []

    for mid in machine_ids:
        url = f"{URLS['machines']}/{mid}/points"
        resp = requests.get(url, headers=headers, verify=False)

        if resp.status_code == 200:
            dados = resp.json()
            for p in dados:
                p["MachineId"] = mid
            pontos.extend(dados)

    return pd.DataFrame(pontos)

# EXECUÇÃO
token = obter_token()
df_point_raw = get_points(token, machine_ids)

# Seleciona apenas colunas desejadas
colunas_desejadas = ['ID', 'Name', 'NodeTypeName', 'MachineId']
df_point = df_point_raw[colunas_desejadas].copy()

"""# **LISTA DE PONTOS**"""

# Extrai lista com os IDs dos ativos filtrados
point_ids = df_point['ID'].tolist()

# Exibe a lista
print(f"{len(point_ids)} pontos encontrados")
print(point_ids)

"""# **PLANILHA POINT ID**"""

# Nome da planilha
planilha_id = "1KbH3fLvOwuk_pJaf2JgdOQ8dnZ0mXyDvmyN8XUl93b8"
nome_da_aba = "Sheet1"

# Abre a planilha
planilha = gc.open_by_key(planilha_id)
aba = planilha.worksheet(nome_da_aba)

# Limpa a aba antes de escrever os dados (opcional)
aba.clear()

# Envia o DataFrame para a aba
set_with_dataframe(aba, df_point)

print("Dados enviados com sucesso para o Google Sheets!")

"""# **REQUISIÇÃO DE ALARMES**"""

def get_alarms(token, machine_ids):
    headers = {"Authorization": f"Bearer {token}", "Accept": "application/json"}
    registros = []

    for mid in machine_ids:
        url = f"{URLS['machines']}/{mid}/points"
        resp = requests.get(url, headers=headers, verify=False)

        if resp.status_code == 200:
            dados_pontos = resp.json()
            for ponto in dados_pontos:
                registro = {
                    "ID": ponto.get("ID"),
                    "HighAlarm": None,
                    "HighWarning": None,
                    "Freq_AlarmLevel": None,
                    "Freq_WarningLevel": None
                }

                # === OverallAlarm ===
                overall_alarm = ponto.get("OverallAlarm")
                summary = overall_alarm.get("Summary", "") if isinstance(overall_alarm, dict) else ""
                if summary:
                    parts = summary.lower().replace(" / ", "/").split("/")
                    for part in parts:
                        if "high alarm" in part:
                            try:
                                registro["HighAlarm"] = float(part.split()[-1].replace(",", ""))
                            except:
                                registro["HighAlarm"] = None
                        elif "high warning" in part:
                            try:
                                registro["HighWarning"] = float(part.split()[-1].replace(",", ""))
                            except:
                                registro["HighWarning"] = None

                # === Frequencies["Overall"] ===
                freq_list = ponto.get("Frequencies", [])
                freq_overall = next((f for f in freq_list if f.get("Frequency") == "Overall"), {})
                alarm_raw = str(freq_overall.get("AlarmLevel", ""))
                warning_raw = str(freq_overall.get("WarningLevel", ""))

                try:
                    registro["Freq_AlarmLevel"] = float(alarm_raw.split()[0].replace(",", ""))
                except:
                    registro["Freq_AlarmLevel"] = None

                try:
                    registro["Freq_WarningLevel"] = float(warning_raw.split()[0].replace(",", ""))
                except:
                    registro["Freq_WarningLevel"] = None

                registros.append(registro)

    return pd.DataFrame(registros)


# === EXECUÇÃO ===
token = obter_token()
df_alarms = get_alarms(token, machine_ids)

print(f"\nTotal de pontos coletados: {len(df_alarms)}")

"""# **PLANILHA ALARMS VALUE**"""

# Nome da planilha
planilha_id = "1Mik0iRW7WiZYMTOiKF-UboXmbSc7YIN4Yi1AfrpDB2o"
nome_da_aba = "Sheet1"

# Abre a planilha
planilha = gc.open_by_key(planilha_id)
aba = planilha.worksheet(nome_da_aba)

# Limpa a aba antes de escrever os dados (opcional)
aba.clear()

# Envia o DataFrame para a aba
set_with_dataframe(aba, df_alarms)

print("Dados enviados com sucesso para o Google Sheets!")

"""# **REQUISIÇÃO DE MEDIÇÕES**"""

def consultar_trends(point_ids, token):
    headers = {
        "Authorization": f"Bearer {token}",
        "Accept": "application/json"
    }

    from_date = datetime.utcnow() - timedelta(days=1)
    to_date = datetime.utcnow()

    params = {
        "fromDateUTC": from_date.isoformat() + "Z",
        "toDateUTC": to_date.isoformat() + "Z"
    }

    resultados = []
    pontos_com_dados = 0

    for raw_pid in point_ids:
        try:
            pid = int(raw_pid)
        except:
            continue

        url = f"{URLS['points']}/{pid}/trendMeasurements"
        response = requests.get(url, headers=headers, params=params, verify=False)

        if response.status_code == 200:
            data = response.json()
            if data:
                pontos_com_dados += 1
                for record in data:
                    base = {
                        "ReadingTimeUTC": record.get("ReadingTimeUTC"),
                        "PointID": record.get("PointID"),
                        "Speed": record.get("Speed"),
                        "SpeedUnits": record.get("SpeedUnits"),
                        "Process": record.get("Process"),
                        "ProcessUnits": record.get("ProcessUnits"),
                        "Digital": record.get("Digital"),
                        "NumberOfChannels": record.get("NumberOfChannels"),
                    }
                    for m in record.get("Measurements", []):
                        resultados.append({
                            **base,
                            "Channel": m.get("Channel"),
                            "ChannelName": m.get("ChannelName"),
                            "Level": m.get("Level"),
                            "Units": m.get("Units"),
                            "BOV": m.get("BOV")
                        })

    df = pd.DataFrame(resultados)

    print(f"\n Total de pontos com dados: {pontos_com_dados}")
    print(f"Total de medições retornadas: {len(df)}")

    return df

if __name__ == "__main__":
    token = obter_token()
    ubu_point_ids_clean = [int(pid) for pid in point_ids if pd.notna(pid)]

    df_trends_ubu = consultar_trends(ubu_point_ids_clean, token)

    if not df_trends_ubu.empty:
        # Filtra por ChannelName desejados
        df_filtrado = df_trends_ubu[
            df_trends_ubu['ChannelName'].isin(['Valor global', 'Overall'])
        ]

        # Seleciona colunas desejadas
        colunas_desejadas = ['ReadingTimeUTC', 'PointID', 'Level', 'Units']
        df_trendMeasurements = df_filtrado[colunas_desejadas].drop_duplicates(subset=colunas_desejadas)

        print(f"{len(df_trendMeasurements)} medições após filtro e remoção de duplicados")

    else:
        print("Nenhuma medição retornada.")

"""# **PLANILHA TREND MEASUREMENTS**"""

# Nome da planilha
planilha_id = "16B1QSbZG-OK8Zs3LiuqG91g_976PcZtmq8kNiVtK4iM"
nome_da_aba = "Sheet1"

# Abre a planilha
planilha = gc.open_by_key(planilha_id)
aba = planilha.worksheet(nome_da_aba)

# Lê os dados atuais da aba (já existentes)
df_existente = get_as_dataframe(aba, evaluate_formulas=True).dropna(how="all")

# Garante que colunas estão no mesmo formato e ordem
colunas_chave = ['ReadingTimeUTC', 'PointID', 'Level', 'Units']
df_existente = df_existente[colunas_chave].dropna()

# Remove duplicados e encontra apenas as linhas novas
df_novos = df_trendMeasurements[~df_trendMeasurements.isin(df_existente.to_dict(orient='list')).all(axis=1)]

# Se houver novos registros, adiciona abaixo
if not df_novos.empty:
    # Número de linhas já existentes (para inserir a partir da próxima linha vazia)
    ultima_linha = len(df_existente) + 2  # +1 para header, +1 para próxima
    set_with_dataframe(aba, df_novos, row=ultima_linha, col=1, include_column_header=False)
    print(f"{len(df_novos)} novas medições adicionadas à planilha!")
else:
    print("Nenhuma medição nova para inserir — tudo já está na planilha.")
